# Freeze fine-tuning configuration for LLaMA-Factory
# Only trains specific parameters (layer norms and head)

# Model and data
stage: sft
model_name_or_path: ckpts/gpt2_small_urank
dataset: alpaca_en
output_dir: outputs/freeze
template: default

# Fine-tuning type - Freeze
finetuning_type: freeze

# Trainable parameters (rest are frozen)
# Patterns are regex-based, matched against parameter names
# Common patterns:
#   - lm_head: final language modeling head
#   - .*layer_norm.*: all layer normalization parameters
#   - .*ln_.*: alternative layer norm naming
freeze_trainable_parameters:
  - "lm_head"
  - ".*layer_norm.*"
  - ".*ln_.*"

# Training hyperparameters
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 5.0e-5
num_train_epochs: 2
lr_scheduler_type: cosine
warmup_ratio: 0.1

# Optimization
bf16: true  # Use bfloat16 if supported
fp16: false
gradient_checkpointing: false  # Not needed for freeze FT

# Logging and saving
logging_steps: 20
save_steps: 1000
save_total_limit: 2
eval_strategy: "no"

# Other
seed: 42